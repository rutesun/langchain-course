import asyncio
import os
import ssl
from typing import Any, Dict, List

import certifi
from dotenv import load_dotenv
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain_tavily import TavilyCrawl, TavilyExtract, TavilyMap
from consts import INDEX_NAME
from pinecone import Pinecone

# logger.pyëŠ” ë¡œê·¸ ì¶œë ¥ì„ ì˜ˆì˜ê²Œ í•˜ê¸° ìœ„í•œ ì‚¬ìš©ì ì •ì˜ ëª¨ë“ˆì…ë‹ˆë‹¤.
from logger import Colors, log_error, log_header, log_info, log_success, log_warning

load_dotenv()

# Configure SSL context to use certifi certificates
ssl_context = ssl.create_default_context(cafile=certifi.where())
os.environ["SSL_CERT_FILE"] = certifi.where()
os.environ["REQUESTS_CA_BUNDLE"] = certifi.where()


# OpenAIEmbeddings: í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ëª¨ë¸ ì„¤ì •
# text-embedding-3-small: ì„±ëŠ¥ê³¼ ë¹„ìš© ë©´ì—ì„œ íš¨ìœ¨ì ì¸ ìµœì‹  ëª¨ë¸
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small",
    api_key=os.getenv("MY_OPENAI_API_KEY"),
    base_url="https://api.openai.com/v1",
    show_progress_bar=False,
    chunk_size=50,
    retry_min_seconds=10,
)

# Chroma: ë¡œì»¬ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•œ ë²¡í„° ì €ì¥ì†Œ (í˜„ì¬ ì½”ë“œì—ì„œëŠ” ì„ ì–¸ë§Œ ë˜ê³  ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
chroma = Chroma(persist_directory="chroma_db", embedding_function=embeddings)

# PineconeVectorStore: í´ë¼ìš°ë“œ ê¸°ë°˜ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤
# index_name: Pinecone ì½˜ì†”ì—ì„œ ìƒì„±í•œ ì¸ë±ìŠ¤ ì´ë¦„ (consts.pyì— ì •ì˜ë¨)
vectorstore = PineconeVectorStore(index_name=INDEX_NAME, embedding=embeddings)

# TavilyCrawl: ì›¹ í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•˜ì—¬ LLMì— ì í•©í•œ í¬ë§·ìœ¼ë¡œ ë³€í™˜í•´ì£¼ëŠ” ë„êµ¬
tavily_crawl = TavilyCrawl()


async def index_documents_async(documents: List[Document], batch_size: int = 50):
    """
    ë¬¸ì„œë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥í•©ë‹ˆë‹¤.
    ëŒ€ëŸ‰ì˜ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•  ë•Œ ì†ë„ë¥¼ ë†’ì´ê³ , API í˜¸ì¶œ ì œí•œì„ ê´€ë¦¬í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    log_header("VECTOR STORAGE PHASE")
    log_info(
        f"ğŸ“š VectorStore Indexing: Preparing to add {len(documents)} documents to vector store",
        Colors.DARKCYAN,
    )

    # Create batches
    batches = [
        documents[i : i + batch_size] for i in range(0, len(documents), batch_size)
    ]

    log_info(
        f"ğŸ“¦ VectorStore Indexing: Split into {len(batches)} batches of {batch_size} documents each"
    )

    # Process all batches concurrently
    async def add_batch(batch: List[Document], batch_num: int):
        try:
            await vectorstore.aadd_documents(batch)
            log_success(
                f"VectorStore Indexing: Successfully added batch {batch_num}/{len(batches)} ({len(batch)} documents)"
            )
        except Exception as e:
            log_error(
                f"VectorStore Indexing: Failed to add batch {batch_num} - {e}",
                Colors.RED,
            )
            return False
        return True

    # Process batches concurrently
    tasks = [add_batch(batch, i + 1) for i, batch in enumerate(batches)]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # Count successful batches
    successful = sum(1 for result in results if result is True)

    if successful == len(batches):
        log_success(
            f"VectorStore Indexing: All batches processed successfully! ({successful}/{len(batches)})"
        )
    else:
        log_warning(
            f"VectorStore Indexing: Processed {successful}/{len(batches)} batches successfully"
        )


async def main():
    """Main async function to orchestrate the entire process."""
    log_header("DOCUMENTATION INGESTION PIPELINE")

    log_info(
        "ğŸ” TavilyCrawl: Starting to crawl documentation from https://docs.langchain.com/oss/python/langchain/",
        Colors.PURPLE,
    )

    # TavilyCrawl.invoke: ì„¤ì •í•œ íŒŒë¼ë¯¸í„°ë¡œ í¬ë¡¤ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    # url: í¬ë¡¤ë§í•  ì‹œì‘ URL
    # extract_depth: "advanced"ë¡œ ì„¤ì •í•˜ì—¬ ê¹Šì´ ìˆëŠ” ë‚´ìš© ì¶”ì¶œ
    # max_depth: ë§í¬ë¥¼ íƒ€ê³  ë“¤ì–´ê°ˆ ê¹Šì´ 30 (í˜„ì¬ í˜ì´ì§€ -> ë§í¬ -> ë§í¬)
    tavily_crawl_results = tavily_crawl.invoke(
        input={
            "url": "https://docs.langchain.com/oss/python/langchain/",
            "extract_depth": "advanced",
            # "instructions": "Documentatin relevant to ai agents",
            "max_depth": 3,
        }
    )
    if tavily_crawl_results.get("error"):
        log_error(f"TavilyCrawl: {tavily_crawl_results['error']}")
        return
    else:
        log_success(
            f"TavilyCrawl: Successfully crawled {len(tavily_crawl_results)} URLs from documentation site"
        )

    all_docs = []
    for tavily_crawl_result_item in tavily_crawl_results["results"]:
        log_info(
            f"TavilyCrawl: Successfully crawled {tavily_crawl_result_item['url']} from documentation site"
        )
        all_docs.append(
            Document(
                page_content=tavily_crawl_result_item["raw_content"] or "",
                metadata={"source": tavily_crawl_result_item["url"]},
            )
        )

    # Split documents into chunks
    log_header("DOCUMENT CHUNKING PHASE")
    log_info(
        f"âœ‚ï¸  Text Splitter: Processing {len(all_docs)} documents with 4000 chunk size and 200 overlap",
        Colors.YELLOW,
    )
    # RecursiveCharacterTextSplitter: ë¬¸ì„œë¥¼ ì‘ì€ ë‹¨ìœ„(ì²­í¬)ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
    # chunk_size=4000: í•œ ì²­í¬ë‹¹ ìµœëŒ€ 4000ì
    # chunk_overlap=200: ì²­í¬ ê°„ 200ìê°€ ê²¹ì¹˜ë„ë¡ í•˜ì—¬ ë¬¸ë§¥ ë‹¨ì ˆ ë°©ì§€
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=4000, chunk_overlap=200)
    splitted_docs = text_splitter.split_documents(all_docs)
    log_success(
        f"Text Splitter: Created {len(splitted_docs)} chunks from {len(all_docs)} documents"
    )

    # Process documents asynchronously
    await index_documents_async(splitted_docs, batch_size=500)

    log_header("PIPELINE COMPLETE")
    log_success("ğŸ‰ Documentation ingestion pipeline finished successfully!")
    log_info("ğŸ“Š Summary:", Colors.BOLD)
    log_info(f"   â€¢ Pages crawled: {len(tavily_crawl_results)}")
    log_info(f"   â€¢ Documents extracted: {len(all_docs)}")
    log_info(f"   â€¢ Chunks created: {len(splitted_docs)}")


if __name__ == "__main__":
    asyncio.run(main())
